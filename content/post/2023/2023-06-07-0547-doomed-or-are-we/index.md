---
# article
title: "We're doomed - or are we?"
subtitle: ""
summary: "Yet another brief and partial attempt to make sense of the current AI debate"
slug: "doomed-or-are-we"
authors: ["synesthesia"]
tags: ["AI", "100DaysToOffload"]
categories: []
date: 2023-06-07T06:47:47+01:00
#lastmod: 2023-06-07T06:47:47+01:00
featured: false
draft: false
type: post


# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---
## A soup of views

There's a lot of talk around the net at the moment about the perils of uncontrolled AI (which for the context of this post mean [Large Language Models](https://garden.synesthesia.co.uk/Large%20Language%20Model)), for example a large group of experts have signed an [open letter calling for a pause in development whilst risks and regulation are explored](https://futureoflife.org/open-letter/pause-giant-ai-experiments).

Another line of warning relates to implicit biases in the content used to train the models (e.g. researchers such as [Bender and Gebru](https://dl.acm.org/doi/10.1145/3442188.3445922) have highlighted the the danger that GPT tools which are trained on *"the internet"* tend to *"risk perpetuating dominant viewpoints, increasing power imbalances and further reifying inequality"*).

Even further, the dominant world-view of the companies and big-tech leaders who are driving the research comes into focus for criticism, for example [Émile P. Torres](https://www.xriskology.com/) and [Timnit Gebru](
-https://dair-community.social/@timnitGebru)  have [coined](https://twitter.com/xriskology/status/1635313845400113153) the acronym [TESCREAL](https://twitter.com/xriskology/status/1635313838508883968?s=20) to describe a [set of right-wing ideologies](https://washingtonspectator.org/understanding-tescreal-silicon-valleys-rightward-turn/) that they see [underlying the motivations of many of the people and companies involved]((https://peopleofcolorintech.com/articles/timnit-gebru-and-emile-torres-call-out-racist-roots-of-the-tech-elites-ai-ideologies/)).

Balancing this are views that all the statements about AI being an 'extinction risk' are too vague (with the exception of possible weaponisation) for effective action to be taken: 
>"I am strongly in favour of being as careful as we possibly can be, and have been saying so publicly for the past ten years, it is important to maintain a sense of proportion – particularly when discussing the extinction of a species of eight billion individuals. AI can create social problems that must really be averted. As scientists, we have a duty to understand them and then do our best to solve them. But the first step is to name and describe them – and to be specific." 
>
> &mdash; [Nello Cristianini](https://theconversation.com/if-were-going-to-label-ai-an-extinction-risk-we-need-to-clarify-how-it-could-happen-206738)

For a complete contrast, Cory Doctorow [sees all of this](https://doctorow.medium.com/ayyyyyy-eyeeeee-4ac92fa2eed) as [criti-hype](https://sts-news.medium.com/youre-doing-it-wrong-notes-on-criticism-and-technology-hype-18b08b4307e5) &mdash;  *criticism that incorporates a self-serving commercial boast*

>If AI is an existential threat to the human race, it is powerful and therefore valuable. The problems with a powerful AI are merely “shakedown” bugs, not showstoppers. Bosses can use AI to replace human workers, even though the AI does a much worse job — just give it a little time and those infelicities will be smoothed over by the wizards who created these incredibly dangerous and therefore powerful tools.

Doctorow thinks that in reality AI is just one more step on the road to the ['enshittificaiton'](https://pluralistic.net/2023/01/21/potemkin-ai/#hey-guys) at the heart of many big-tech business models, or in [other words](https://aus.social/@PaulWay/110496947991359261) this is just one more example of how late-stage capitalism works, *'the corporations that use AI try to take over the world'*.

To borrow from [Harold Jarche](https://mastodon.social/@harold/110499816999861929): 

{{< figure src="jarche-tetrad-capitalism.png" caption="A McLuhan tetrad on Capitalism - (c) Harold Jarche - CC BY-NC-SA 4.0" numbered="true" >}}

## So what can an individual do?

Heuristically, the usual basic tenets of making sense of new technologies and the hype around them would seem to apply:

- learn *'enough'* about what it is and the basics of how it does what it does
- think about the implied world-view in any statement
- follow the money
- find narrow use-cases where it seems likely the benefits out-weigh the risks, and experiment

## Where am I experimenting currently?

The following areas seem useful to me at the moment:

- I am using [GitHub CoPilot](https://docs.github.com/en/copilot/getting-started-with-github-copilot) as an aid for both personal and professional software writing
- where I work, I have guided the sales team to get going with Microsoft [Viva Sales](https://www.microsoft.com/en-us/microsoft-viva/sales#overview) within Dynamics 365
- I am occasionally playing with ChatGPT and similar to put together first drafts of things
- very likely to experiment with Microsoft 365 CoPilot once it is available to us

## A tentative conclusion, for now

Overall my sense is it's too soon to say (in terms specifically of the LLM tools - there are plenty of other examples in the wider sphere of AI which are already proven sources of benefit).

Unlike other over-hyped technologies such as blockchain and crypto, this one looks like it might be genuinely useful in certain scenarios.

As always, **caveat emptor**.

[#100DaysToOffload](https://100daystooffload.com/) 37/100

<ins datetime="2023-06-08T06:27:00TZD">08/06/2023 Minor edit for style</ins>
